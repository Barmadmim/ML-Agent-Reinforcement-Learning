Escenario: Juego Simple de Grid 3x3
Imaginemos un entorno de 3x3 como en el ejemplo anterior. El agente se mueve en una cuadrÃ­cula, tratando de llegar a un objetivo (celda 8) desde una posiciÃ³n inicial (celda 0). El agente recibe recompensas cuando llega al objetivo (+10) y penalizaciones (-1) por moverse y -5 si se sale del grid.

Pero esta vez no vamos a usar una tabla Q, sino que el agente utilizarÃ¡ una red neuronal profunda (Deep Q-Network) para predecir los valores Q de las acciones.

1. DefiniciÃ³n de Estados y Acciones:
Estados: Cada celda en el grid representa un estado. Tenemos 9 estados (numerados de 0 a 8), pero no usaremos una tabla Q para cada estado. En cambio, introduciremos los estados como vectores de entrada en la red neuronal.

Por ejemplo, el estado 0 (posiciÃ³n inicial) se representa como un vector:

ğ‘ 
0
=
[
1
,
0
,
0
,
0
,
0
,
0
,
0
,
0
,
0
]
s 
0
â€‹
 =[1,0,0,0,0,0,0,0,0]
El estado 1 serÃ¡:

ğ‘ 
1
=
[
0
,
1
,
0
,
0
,
0
,
0
,
0
,
0
,
0
]
s 
1
â€‹
 =[0,1,0,0,0,0,0,0,0]
Y asÃ­ sucesivamente.

Acciones: El agente puede moverse en 4 direcciones:

Arriba (â†‘), Abajo (â†“), Izquierda (â†), Derecha (â†’).
2. Recompensas:
Recompensa de +10 al llegar al estado objetivo (celda 8).
PenalizaciÃ³n de -1 por moverse.
PenalizaciÃ³n de -5 si se sale del grid.
3. Estructura de la Red Neuronal (Deep Q-Network):
La Deep Q-Network (DQN) serÃ¡ una red neuronal simple con:

Entrada: El estado del entorno, representado como un vector de 9 dimensiones.
Capas ocultas: Capas densas con activaciones ReLU.
Salida: Un vector de 4 dimensiones, que representa el valor Q estimado para cada una de las 4 acciones posibles (arriba, abajo, izquierda, derecha).
Red DQN:
Entrada: Vector de tamaÃ±o 9 (representa el estado).
Capa oculta 1: 64 neuronas, activaciÃ³n ReLU.
Capa oculta 2: 64 neuronas, activaciÃ³n ReLU.
Capa de salida: 4 neuronas, sin activaciÃ³n. Cada neurona da el valor Q para una acciÃ³n posible (arriba, abajo, izquierda, derecha).
Dimensi
o
ËŠ
nÂ deÂ entrada
â†’
CapaÂ ocultaÂ 1
â†’
CapaÂ ocultaÂ 2
â†’
CapaÂ deÂ salida
Dimensi 
o
ËŠ
 nÂ deÂ entradaâ†’CapaÂ ocultaÂ 1â†’CapaÂ ocultaÂ 2â†’CapaÂ deÂ salida
9
â†’
64
â†’
64
â†’
4
9â†’64â†’64â†’4
4. Entrenamiento del DQN:
Datos de entrenamiento:
Input: El estado actual (como vector de 9 valores, uno por cada celda del grid).
Output: El valor Q para cada acciÃ³n posible en ese estado.
El algoritmo aprende a ajustar los pesos de la red para predecir correctamente los valores Q, utilizando la fÃ³rmula de actualizaciÃ³n del Q-Learning:

ğ‘„
(
ğ‘ 
,
ğ‘
)
â†
ğ‘„
(
ğ‘ 
,
ğ‘
)
+
ğ›¼
(
ğ‘Ÿ
+
ğ›¾
max
â¡
ğ‘
â€²
ğ‘„
(
ğ‘ 
â€²
,
ğ‘
â€²
)
âˆ’
ğ‘„
(
ğ‘ 
,
ğ‘
)
)
Q(s,a)â†Q(s,a)+Î±(r+Î³ 
a 
â€²
 
max
â€‹
 Q(s 
â€²
 ,a 
â€²
 )âˆ’Q(s,a))
Donde:

s: Estado actual.
a: AcciÃ³n tomada.
r: Recompensa obtenida.
s': Nuevo estado.
ğ›¾
Î³: Factor de descuento.
ğ›¼
Î±: Tasa de aprendizaje.
Ejemplo concreto:
Estado actual: El agente estÃ¡ en la celda 0, representado por el vector:
ğ‘ 
0
=
[
1
,
0
,
0
,
0
,
0
,
0
,
0
,
0
,
0
]
s 
0
â€‹
 =[1,0,0,0,0,0,0,0,0]
Acciones posibles: El agente puede moverse en 4 direcciones.

La red predice los valores Q para cada acciÃ³n. Supongamos que, al principio, los valores Q son los siguientes:

ğ‘„
(
ğ‘ 
0
)
=
[
âˆ’
0.5
0.2
0.1
0.4
]
Q(s 
0
â€‹
 )=[ 
âˆ’0.5
â€‹
  
0.2
â€‹
  
0.1
â€‹
  
0.4
â€‹
 ]
Donde:

Q(s_0, arriba) = -0.5
Q(s_0, abajo) = 0.2
Q(s_0, izquierda) = 0.1
Q(s_0, derecha) = 0.4
Entonces, el agente elige la acciÃ³n con el valor Q mÃ¡s alto (derecha, con 0.4).

Nueva acciÃ³n: El agente se mueve hacia la derecha (acciÃ³n "derecha"). Ahora estÃ¡ en el estado 1 (celda 1).

Recompensa: Por moverse, el agente recibe una penalizaciÃ³n de -1.

ActualizaciÃ³n de Q: La red neuronal se actualiza de acuerdo a la fÃ³rmula de Q-Learning:

ğ‘„
(
ğ‘ 
0
,
derecha
)
â†
ğ‘„
(
ğ‘ 
0
,
derecha
)
+
ğ›¼
(
ğ‘Ÿ
+
ğ›¾
max
â¡
ğ‘
â€²
ğ‘„
(
ğ‘ 
1
,
ğ‘
â€²
)
âˆ’
ğ‘„
(
ğ‘ 
0
,
derecha
)
)
Q(s 
0
â€‹
 ,derecha)â†Q(s 
0
â€‹
 ,derecha)+Î±(r+Î³ 
a 
â€²
 
max
â€‹
 Q(s 
1
â€‹
 ,a 
â€²
 )âˆ’Q(s 
0
â€‹
 ,derecha))
Supongamos:

ğ›¼
=
0.1
Î±=0.1
ğ›¾
=
0.9
Î³=0.9
ğ‘Ÿ
=
âˆ’
1
r=âˆ’1
La red predice los valores Q para el estado 
ğ‘ 
1
s 
1
â€‹
  (celda 1), que inicialmente son:

ğ‘„
(
ğ‘ 
1
)
=
[
0.1
âˆ’
0.2
0.3
0.5
]
Q(s 
1
â€‹
 )=[ 
0.1
â€‹
  
âˆ’0.2
â€‹
  
0.3
â€‹
  
0.5
â€‹
 ]
El valor mÃ¡ximo para 
ğ‘„
(
ğ‘ 
1
)
Q(s 
1
â€‹
 ) es 
0.5
0.5 (acciÃ³n "derecha").
Ahora actualizamos el valor 
ğ‘„
(
ğ‘ 
0
,
derecha
)
Q(s 
0
â€‹
 ,derecha):

ğ‘„
(
ğ‘ 
0
,
derecha
)
=
0.4
+
0.1
Ã—
(
âˆ’
1
+
0.9
Ã—
0.5
âˆ’
0.4
)
Q(s 
0
â€‹
 ,derecha)=0.4+0.1Ã—(âˆ’1+0.9Ã—0.5âˆ’0.4)
Calculamos:

ğ‘„
(
ğ‘ 
0
,
derecha
)
=
0.4
+
0.1
Ã—
(
âˆ’
1
+
0.45
âˆ’
0.4
)
Q(s 
0
â€‹
 ,derecha)=0.4+0.1Ã—(âˆ’1+0.45âˆ’0.4)
ğ‘„
(
ğ‘ 
0
,
derecha
)
=
0.4
+
0.1
Ã—
(
âˆ’
0.95
)
Q(s 
0
â€‹
 ,derecha)=0.4+0.1Ã—(âˆ’0.95)
ğ‘„
(
ğ‘ 
0
,
derecha
)
=
0.4
âˆ’
0.095
=
0.305
Q(s 
0
â€‹
 ,derecha)=0.4âˆ’0.095=0.305
El nuevo valor de 
ğ‘„
(
ğ‘ 
0
,
derecha
)
Q(s 
0
â€‹
 ,derecha) es 0.305.

5. RepeticiÃ³n del proceso:
Este proceso de explorar el entorno, predecir valores Q, tomar acciones, recibir recompensas y actualizar la red se repite miles de veces. Con el tiempo, el agente aprende a tomar las mejores decisiones en cada estado para maximizar las recompensas a largo plazo.

6. Entrenamiento con experiencia replay:
Para estabilizar el entrenamiento, utilizamos una tÃ©cnica llamada replay buffer:

Almacenamos las experiencias del agente (estado, acciÃ³n, recompensa, nuevo estado) en una memoria.
Durante el entrenamiento, seleccionamos muestras aleatorias de esta memoria y las usamos para actualizar la red, en lugar de hacerlo en tiempo real.
Esto mejora la estabilidad y eficiencia del entrenamiento.

7. Salida de la red:
Cada vez que el agente se encuentra en un estado, la red neuronal toma ese estado como entrada y produce un vector de 4 valores Q (uno por cada acciÃ³n posible). El agente elige la acciÃ³n con el valor Q mÃ¡s alto.

Resumen del Proceso DQN:
Input: El estado actual del entorno (como un vector).
Red neuronal: Predice los valores Q para todas las acciones posibles desde ese estado.
SelecciÃ³n de acciÃ³n: Elegimos la acciÃ³n con el mayor valor Q.
ActualizaciÃ³n: DespuÃ©s de tomar una acciÃ³n y recibir una recompensa, se ajustan los pesos de la red para mejorar las predicciones futuras.
